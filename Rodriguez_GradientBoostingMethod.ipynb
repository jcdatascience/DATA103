{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rUOWsbJ6d9fY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cxRTCy2Rf5mg"
      },
      "outputs": [],
      "source": [
        "#load data set\n",
        "url = \"healthcare-dataset-stroke-data.csv\"\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQSZGVAogBPN",
        "outputId": "b2eef974-2c41-437c-c4cd-05ae23d24104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
            "0   9046    Male  67.0             0              1          Yes   \n",
            "1  51676  Female  61.0             0              0          Yes   \n",
            "2  31112    Male  80.0             0              1          Yes   \n",
            "3  60182  Female  49.0             0              0          Yes   \n",
            "4   1665  Female  79.0             1              0          Yes   \n",
            "\n",
            "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
            "0        Private          Urban             228.69  36.6  formerly smoked   \n",
            "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
            "2        Private          Rural             105.92  32.5     never smoked   \n",
            "3        Private          Urban             171.23  34.4           smokes   \n",
            "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
            "\n",
            "   stroke  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RygV7GHRgDAI",
        "outputId": "de597473-aa83-4b3e-dd93-18aa34122feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   gender             5110 non-null   object \n",
            " 2   age                5110 non-null   float64\n",
            " 3   hypertension       5110 non-null   int64  \n",
            " 4   heart_disease      5110 non-null   int64  \n",
            " 5   ever_married       5110 non-null   object \n",
            " 6   work_type          5110 non-null   object \n",
            " 7   Residence_type     5110 non-null   object \n",
            " 8   avg_glucose_level  5110 non-null   float64\n",
            " 9   bmi                4909 non-null   float64\n",
            " 10  smoking_status     5110 non-null   object \n",
            " 11  stroke             5110 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 479.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1PzQB6igGmF",
        "outputId": "fedd7e46-52ff-4a60-9472-dd1a2966c667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 null values in the id column\n",
            "There are 0 null values in the gender column\n",
            "There are 0 null values in the age column\n",
            "There are 0 null values in the hypertension column\n",
            "There are 0 null values in the heart_disease column\n",
            "There are 0 null values in the ever_married column\n",
            "There are 0 null values in the work_type column\n",
            "There are 0 null values in the Residence_type column\n",
            "There are 0 null values in the avg_glucose_level column\n",
            "There are 201 null values in the bmi column\n",
            "There are 0 null values in the smoking_status column\n",
            "There are 0 null values in the stroke column\n"
          ]
        }
      ],
      "source": [
        "# checking for the null values of each column\n",
        "\n",
        "for each in data.columns:\n",
        "    print(f'There are {data[each].isnull().sum()} null values in the {each} column')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RDk_TAWuKnXe"
      },
      "outputs": [],
      "source": [
        "data.dropna(inplace=True)\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['stroke'])\n",
        "y = data['stroke']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWPNrTyiZMuT"
      },
      "source": [
        "Loading and Cleaning Data\n",
        "- We load the dataset and remove rows with missing values using dropna.\n",
        "- We use pd.get_dummies to convert categorical columns into numerical representations for easier processing.\n",
        "\n",
        "Feature and Target Selection\n",
        "- We define X as the features and y as the target (stroke).\n",
        "\n",
        "Data Splitting\n",
        "- Using train_test_split, we create training and testing sets with 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2ByGaMfvXVbW"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeCustom:\n",
        "    def __init__(self, maxDepth=3):\n",
        "        self.maxDepth = maxDepth\n",
        "        self.tree = []\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        if depth < self.maxDepth:\n",
        "            best_mse = float('inf')\n",
        "            n_samples, n_features = X.shape\n",
        "            for feature in range(n_features):\n",
        "                feature_vals = np.unique(X[:, feature])\n",
        "                for threshold in feature_vals:\n",
        "                    preds = np.where(X[:, feature] > threshold,\n",
        "                                     np.full(y.shape, y.mean()),\n",
        "                                     np.full(y.shape, y.mean() - y.std()))\n",
        "                    mse = np.mean((y - preds) ** 2)\n",
        "                    if mse < best_mse:\n",
        "                        best_mse = mse\n",
        "                        self.featureIndex = feature\n",
        "                        self.threshold = threshold\n",
        "            self.tree.append((self.featureIndex, self.threshold))\n",
        "\n",
        "    def predict(self, X):\n",
        "        feature_vals = X[:, self.featureIndex]\n",
        "        return np.where(feature_vals > self.threshold, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJJWob0yY6Ss"
      },
      "source": [
        "Decision Stump Attributes\n",
        "\n",
        "- featureIndex: the feature to split on.\n",
        "- threshold: the value to decide the split.\n",
        "- polarity: not used in this version, but typically allows for adjusting direction of split.\n",
        "\n",
        "\n",
        "Finding the Best Split\n",
        "- We loop through each feature and each unique value in that feature to find the best threshold based on mean squared error (MSE).\n",
        "- We store the feature and threshold with the lowest MSE.\n",
        "\n",
        "Prediction\n",
        "- Given input data, predict checks if each value is above the threshold and returns a binary prediction (1 or 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J_P43vx3YEo3"
      },
      "outputs": [],
      "source": [
        "class GradientBoostingClassifier:\n",
        "    def __init__(self, numEstimators=50, learningRate=0.05, maxDepth=4):\n",
        "        self.numEstimators = numEstimators\n",
        "        self.learningRate = learningRate\n",
        "        self.maxDepth = maxDepth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        preds = np.full(y.shape, y.mean())\n",
        "\n",
        "        for i in range(self.numEstimators):\n",
        "            residuals = y - preds\n",
        "            print(f\"Iteration {i + 1}\")\n",
        "            print(\"Residuals:\\n\", residuals[:5])\n",
        "\n",
        "            tree = DecisionTreeCustom(maxDepth=self.maxDepth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            tree_preds = tree.predict(X)\n",
        "            preds += self.learningRate * tree_preds\n",
        "\n",
        "            print(\"Updated Predictions:\", preds[:5])\n",
        "            print(\"-\" * 40)\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learningRate * tree.predict(X)\n",
        "        return np.where(y_pred > 0.5, 1, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FsCQzjfYtdK"
      },
      "source": [
        "__init__\n",
        "- it initializes parameters like numEstimators, learningRate, and maxDepth\n",
        "\n",
        "fit method\n",
        "- it starts with initial predictions set to the mean of y\n",
        "- for each iteration, it calculates residuals (the difference between actual values and predictions)\n",
        "- it also fits a simple tree on these residuals, appending it to self.trees\n",
        "- finally it updates predictions by adding the treeâ€™s predictions, scaled by the learningRate\n",
        "\n",
        "predict method\n",
        "\n",
        "- uses the learned trees to make predictions on new data by summing up predictions from each tree, weighted by learningRate\n",
        "- then it converts the result to a binary prediction (0 or 1) based on a threshold of 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_boosting = GradientBoostingClassifier(numEstimators=5, learningRate=0.05, maxDepth=4)\n",
        "gradient_boosting.fit(X_train, y_train)\n",
        "\n",
        "# predict on the test set then evaluate\n",
        "y_pred = gradient_boosting.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDU1ds2jNo2J",
        "outputId": "e327b96f-fa55-4444-db34-62ece9f84143"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Residuals:\n",
            " 3565   -0.039725\n",
            "898    -0.039725\n",
            "2707   -0.039725\n",
            "4198   -0.039725\n",
            "2746   -0.039725\n",
            "Name: stroke, dtype: float64\n",
            "Updated Predictions: [0.08972498 0.08972498 0.08972498 0.08972498 0.08972498]\n",
            "----------------------------------------\n",
            "Iteration 2\n",
            "Residuals:\n",
            " 3565   -0.089725\n",
            "898    -0.089725\n",
            "2707   -0.089725\n",
            "4198   -0.089725\n",
            "2746   -0.089725\n",
            "Name: stroke, dtype: float64\n",
            "Updated Predictions: [0.13972498 0.13972498 0.13972498 0.13972498 0.13972498]\n",
            "----------------------------------------\n",
            "Iteration 3\n",
            "Residuals:\n",
            " 3565   -0.139725\n",
            "898    -0.139725\n",
            "2707   -0.139725\n",
            "4198   -0.139725\n",
            "2746   -0.139725\n",
            "Name: stroke, dtype: float64\n",
            "Updated Predictions: [0.18972498 0.18972498 0.18972498 0.18972498 0.18972498]\n",
            "----------------------------------------\n",
            "Iteration 4\n",
            "Residuals:\n",
            " 3565   -0.189725\n",
            "898    -0.189725\n",
            "2707   -0.189725\n",
            "4198   -0.189725\n",
            "2746   -0.189725\n",
            "Name: stroke, dtype: float64\n",
            "Updated Predictions: [0.23972498 0.23972498 0.23972498 0.23972498 0.23972498]\n",
            "----------------------------------------\n",
            "Iteration 5\n",
            "Residuals:\n",
            " 3565   -0.239725\n",
            "898    -0.239725\n",
            "2707   -0.239725\n",
            "4198   -0.239725\n",
            "2746   -0.239725\n",
            "Name: stroke, dtype: float64\n",
            "Updated Predictions: [0.28972498 0.28972498 0.28972498 0.28972498 0.28972498]\n",
            "----------------------------------------\n",
            "Accuracy: 0.9460285132382892\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97       929\n",
            "           1       1.00      0.00      0.00        53\n",
            "\n",
            "    accuracy                           0.95       982\n",
            "   macro avg       0.97      0.50      0.49       982\n",
            "weighted avg       0.95      0.95      0.92       982\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradient_boosting.fit(X_train, y_train)\n",
        "- it trains the gradient boosting model on the training data by building multiple trees to minimize prediction errors\n",
        "\n",
        "y_pred = gradient_boosting.predict(X_test)\n",
        "- it uses the trained model to predict labels for the test set\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "- calculates accuracy as the percentage of correct predictions.\n",
        "\n",
        "classification_report(y_test, y_pred, zero_division=1)\n",
        "- it just prints metrics like precision, recall, and F1-score, which give a more complete view of model performance, especially when dealing with imbalanced classes\n",
        "\n",
        "\n",
        "for the output.\n",
        "\n",
        "Accuracy: Shows the proportion of correctly predicted instances.\n",
        "Precision, Recall, and F1-score: For each class (0 and 1), these metrics help assess how well the model detects strokes (1), which may not be well captured by accuracy alone."
      ],
      "metadata": {
        "id": "ckmV0V63xSJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt9o8TtIaVd_"
      },
      "source": [
        "# DESCRIPTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WoJHQ8RaZsA"
      },
      "source": [
        "gradient boosting is an ensemble technique that iteratively constructs a number of small models, frequently decision stumps, each of which fixes mistakes caused by the models before it. by incorporating new models that concentrate on residual errors, the objective is to reduce the discrepancy between expected and actual results.\n",
        "\n",
        "at each iteration the algorithm does as follows:\n",
        "\n",
        "- determines the residuals, or the discrepancies between the current projections and the actual target values.\n",
        "- fits these residuals to a weak learner (such a decision stump).\n",
        "- adds the new learner's predictions to the existing ones, scaling them according to the learning rate, a hyperparameter that regulates each learner's contribution.\n",
        "\n",
        "\n",
        "until the residual errors are reduced, or for a predetermined number of repetitions, this process is repeated. the final model can capture intricate relationships in the data since it is a weighted sum of all the weak learners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzZbN0-xbSlE"
      },
      "source": [
        "# Pseudocode for Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YJwo_HbXra"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Initialize predictions to the mean of the target values (y_mean)\n",
        "\n",
        "FOR each iteration up to the number of estimators:\n",
        "    Calculate residuals = (target values - current predictions)\n",
        "    \n",
        "    Train a weak learner (e.g., decision stump) on the residuals\n",
        "    \n",
        "    Add the weak learner's predictions to the current predictions,\n",
        "    scaled by the learning rate\n",
        "    \n",
        "END FOR\n",
        "\n",
        "RETURN final predictions based on the sum of all weak learners' contributions\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9epwXHPWbmrB"
      },
      "source": [
        "Initialization\n",
        "- To provide a baseline, begin with a straightforward prediction, such as the target variable's mean.\n",
        "\n",
        "Residual Calculation\n",
        "- Determine the difference between the current predictions and the actual target values for each iteration.\n",
        "\n",
        "Weak Learner Training\n",
        "- Teach a basic model, or weak learner, to forecast these residuals.\n",
        "\n",
        "Update Predictions\n",
        "- Adding the new learner's predictions, weighted by a learning rate, will update the forecasts.\n",
        "\n",
        "Repeat\n",
        "- Keep going until every student has been added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1gipEMvb6bZ"
      },
      "source": [
        "# Differences Between Gradient Boosting and Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzLqqGrOb9FE"
      },
      "source": [
        "1. Model Combination\n",
        "\n",
        "- With gradient boosting, models are constructed one after the other, with each new model concentrating on the residual errors of the ones that came before it. We refer to this methodical process as \"boosting.\"\n",
        "- In contrast, Random Forests construct each tree separately before averaging their predictions to produce a final outcome. We refer to this parallel strategy as \"bagging.\"\n",
        "\n",
        "2. Prediction Strategy\n",
        "\n",
        "- Gradient Boosting generates predictions by adding together all of the weak learners' predictions, each of which is weighted by a learning rate.\n",
        "- Random Forests use the majority vote (for classification) or average (for regression) across all trees to generate predictions.\n",
        "\n",
        "3. Complexity and Interpretability\n",
        "\n",
        "- Because each weak learner concentrates on fixing certain mistakes from the previous stage, gradient boosting frequently results in a more complex final model. Although it needs to be carefully adjusted (e.g., the number of iterations and learning rate), it typically performs better on more difficult issues.\n",
        "- Because Random Forests average independent trees, they are easier to understand and more resilient to overfitting, which allows them to be applied to a greater range of situations without requiring a lot of fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrwkCqiOciay"
      },
      "source": [
        "In short, Random Forests are parallel and concentrate on averaging results from independent models, whereas Gradient Boosting is sequential and modifies each model based on the errors of the previous one."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}